{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Retrieval Augmented Question & Answering with Amazon Bedrock using LangChain\n\n### Challenges\n\nWhen trying to solve a Question Answering task over a larger document corpus with the help of LLMs we need to master the following challenges (amongst others):\n- How to manage large document(s) that exceed the token limit\n- How to find the document(s) relevant to the question being asked\n\n### Infusing knowledge into LLM-powered systems\n\nWe have two primary types of knowledge for LLMs:\n- **Parametric knowledge**: refers to everything the LLM learned during training and acts as a frozen snapshot of the world for the LLM. \n- **Source knowledge**: covers any information fed into the LLM via the input prompt. \n\nWhen trying to infuse knowledge into a generative AI - powered application we need to choose which of these types to target. Fine-tuning, explored in other workshops, deals with elevating the parametric knowledge through fine-tuning. Since fine-tuning is a resouce intensive operation, this option is well suited for infusing static domain-specific information like domain-specific langauage/writing styles (medical domain, science domain, ...) or optimizing performance towards a very specific task (classification, sentiment analysis, RLHF, instruction-finetuning, ...). \n\nIn contrast to that, targeting the source knowledge for domain-specific performance uplift is very well suited for all kinds of dynamic information, from knowledge bases in structured and unstructured form up to integration of information from live systems. This Lab is about retrieval-augmented generation, a common design pattern for ingesting domain-specific information through the source knowledge. It is particularily well suited for ingestion of information in form of unstructured text with semi-frequent update cycles. \n\nIn this notebook we explain how to utilize the RAG (retrieval-agumented generation) pattern originating from [this](https://arxiv.org/pdf/2005.11401.pdf) paper published by Lewis et al in 2021. It is particularily useful for Question Answering by finding and leveraging the most useful excerpts of documents out of a larger document corpus providing answers to the user questions.\n\n#### Prepare documents\n![Embeddings](https://singlestoredevday.s3.amazonaws.com/images/RAG_pipeline_S2_Bedrock.png)\n\nBefore being able to answer the questions, the documents must be processed and a stored in a document store index\n- Load the documents\n- Process and split them into smaller chunks\n- Create a numerical vector representation of each chunk using Amazon Bedrock Titan Embeddings model\n- Create an index using the chunks and the corresponding embeddings\n#### Ask question\n![Question](https://singlestoredevday.s3.amazonaws.com/images/RAG_runtime_S2_Bedrock.png)\n\nWhen the documents index is prepared, you are ready to ask the questions and relevant documents will be fetched based on the question being asked. Following steps will be executed.\n- Create an embedding of the input question\n- Compare the question embedding with the embeddings in the index\n- Fetch the (top N) relevant document chunks\n- Add those chunks as part of the context in the prompt\n- Send the prompt to the model under Amazon Bedrock\n- Get the contextual answer based on the documents retrieved"},{"cell_type":"markdown","metadata":{},"source":"## Usecase\n#### Dataset\nIn this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on."},{"cell_type":"markdown","metadata":{},"source":"## Implementation\nIn order to follow the RAG approach this notebook is using the LangChain framework where it has integrations with different services and tools that allow efficient building of patterns such as RAG. We will be using the following tools:\n\n- **LLM (Large Language Model)**: Anthropic Claude available through Amazon Bedrock\n\n  This model will be used to understand the document chunks and provide an answer in human friendly manner.\n- **Embeddings Model**: Amazon Titan Embeddings available through Amazon Bedrock\n\n  This model will be used to generate a numerical representation of the textual documents\n\n- **Document Loader**: \n    - PDF Loader available through LangChain for PDFs\n\n  These are loaders that can load the documents from a source, for the sake of this notebook we are loading the sample files from a local path. This could easily be replaced with a loader to load documents from enterprise internal systems.\n\n- **Vector Store**: SingleStoreDB available through LangChain\n  In this notebook we are using SingleStoreDB to store both the embeddings and the documents.\n\nThen begin with instantiating the LLM and the Embeddings model. Here we are using Anthropic Claude to demonstrate the use case.\n\nNote: It is possible to choose other models available with Bedrock. You can replace the `model_id` as follows to change the model.\n\n`llm = Bedrock(model_id=\"...\")`"},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:33:57.035934Z","iopub.status.busy":"2023-11-13T21:33:57.035684Z","iopub.status.idle":"2023-11-13T21:34:07.204767Z","shell.execute_reply":"2023-11-13T21:34:07.204092Z","shell.execute_reply.started":"2023-11-13T21:33:57.035916Z"},"language":"python","tags":[],"trusted":true},"outputs":[],"source":"!pip install boto3 langchain pypdf tiktoken sqlalchemy --upgrade --quiet"},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:34:11.322734Z","iopub.status.busy":"2023-11-13T21:34:11.322229Z","iopub.status.idle":"2023-11-13T21:34:33.520094Z","shell.execute_reply":"2023-11-13T21:34:33.519600Z","shell.execute_reply.started":"2023-11-13T21:34:11.322707Z"},"language":"python","trusted":true},"outputs":[{"name":"stdin","output_type":"stream","text":"AWS_ACCESS_KEY_ID:  ········\nAWS_SECRET_ACCESS_KEY:  ········\n"}],"source":"import getpass\n\nos.environ['AWS_DEFAULT_REGION']='us-east-1'\nos.environ['AWS_ACCESS_KEY_ID']= getpass.getpass(\"AWS_ACCESS_KEY_ID: \")\nos.environ['AWS_SECRET_ACCESS_KEY']=getpass.getpass(\"AWS_SECRET_ACCESS_KEY: \")"},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:34:37.360777Z","iopub.status.busy":"2023-11-13T21:34:37.360354Z","iopub.status.idle":"2023-11-13T21:34:37.417326Z","shell.execute_reply":"2023-11-13T21:34:37.416877Z","shell.execute_reply.started":"2023-11-13T21:34:37.360758Z"},"language":"python","trusted":true},"outputs":[],"source":"import boto3\nimport json\nimport sys"},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:34:42.888987Z","iopub.status.busy":"2023-11-13T21:34:42.888423Z","iopub.status.idle":"2023-11-13T21:34:42.892683Z","shell.execute_reply":"2023-11-13T21:34:42.892151Z","shell.execute_reply.started":"2023-11-13T21:34:42.888966Z"},"language":"python","trusted":true},"outputs":[],"source":"from io import StringIO\nimport sys\nimport textwrap\n\n\ndef print_ww(*args, width: int = 100, **kwargs):\n    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n    buffer = StringIO()\n    try:\n        _stdout = sys.stdout\n        sys.stdout = buffer\n        print(*args, **kwargs)\n        output = buffer.getvalue()\n    finally:\n        sys.stdout = _stdout\n    for line in output.splitlines():\n        print(\"\\n\".join(textwrap.wrap(line, width=width)))"},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:34:47.323394Z","iopub.status.busy":"2023-11-13T21:34:47.323037Z","iopub.status.idle":"2023-11-13T21:34:47.333660Z","shell.execute_reply":"2023-11-13T21:34:47.333094Z","shell.execute_reply.started":"2023-11-13T21:34:47.323375Z"},"language":"python","trusted":true},"outputs":[],"source":"session = boto3.session.Session()"},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:34:51.652630Z","iopub.status.busy":"2023-11-13T21:34:51.652265Z","iopub.status.idle":"2023-11-13T21:34:51.740887Z","shell.execute_reply":"2023-11-13T21:34:51.740384Z","shell.execute_reply.started":"2023-11-13T21:34:51.652611Z"},"language":"python","tags":[],"trusted":true},"outputs":[{"data":{"text/plain":"{'ResponseMetadata': {'RequestId': 'f6b7f65d-e385-4125-a0cc-181f3f1bcec5',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'date': 'Mon, 13 Nov 2023 21:34:51 GMT',\n   'content-type': 'application/json',\n   'content-length': '8260',\n   'connection': 'keep-alive',\n   'x-amzn-requestid': 'f6b7f65d-e385-4125-a0cc-181f3f1bcec5'},\n  'RetryAttempts': 0},\n 'modelSummaries': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-tg1-large',\n   'modelId': 'amazon.titan-tg1-large',\n   'modelName': 'Titan Text Large',\n   'providerName': 'Amazon',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': True,\n   'customizationsSupported': ['FINE_TUNING'],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-g1-text-02',\n   'modelId': 'amazon.titan-embed-g1-text-02',\n   'modelName': 'Titan Text Embeddings v2',\n   'providerName': 'Amazon',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['EMBEDDING'],\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-lite-v1',\n   'modelId': 'amazon.titan-text-lite-v1',\n   'modelName': 'Titan Text G1 - Lite',\n   'providerName': 'Amazon',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': True,\n   'customizationsSupported': ['FINE_TUNING'],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1',\n   'modelId': 'amazon.titan-text-express-v1',\n   'modelName': 'Titan Text G1 - Express',\n   'providerName': 'Amazon',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': True,\n   'customizationsSupported': ['FINE_TUNING'],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1',\n   'modelId': 'amazon.titan-embed-text-v1',\n   'modelName': 'Titan Embeddings G1 - Text',\n   'providerName': 'Amazon',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['EMBEDDING'],\n   'responseStreamingSupported': True,\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl',\n   'modelId': 'stability.stable-diffusion-xl',\n   'modelName': 'SDXL 0.8',\n   'providerName': 'Stability AI',\n   'inputModalities': ['TEXT', 'IMAGE'],\n   'outputModalities': ['IMAGE'],\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v0',\n   'modelId': 'stability.stable-diffusion-xl-v0',\n   'modelName': 'SDXL 0.8',\n   'providerName': 'Stability AI',\n   'inputModalities': ['TEXT', 'IMAGE'],\n   'outputModalities': ['IMAGE'],\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v1',\n   'modelId': 'stability.stable-diffusion-xl-v1',\n   'modelName': 'SDXL 1.0',\n   'providerName': 'Stability AI',\n   'inputModalities': ['TEXT', 'IMAGE'],\n   'outputModalities': ['IMAGE'],\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-grande-instruct',\n   'modelId': 'ai21.j2-grande-instruct',\n   'modelName': 'J2 Grande Instruct',\n   'providerName': 'AI21 Labs',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': False,\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-jumbo-instruct',\n   'modelId': 'ai21.j2-jumbo-instruct',\n   'modelName': 'J2 Jumbo Instruct',\n   'providerName': 'AI21 Labs',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': False,\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-mid',\n   'modelId': 'ai21.j2-mid',\n   'modelName': 'Jurassic-2 Mid',\n   'providerName': 'AI21 Labs',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': False,\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-mid-v1',\n   'modelId': 'ai21.j2-mid-v1',\n   'modelName': 'Jurassic-2 Mid',\n   'providerName': 'AI21 Labs',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': False,\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra',\n   'modelId': 'ai21.j2-ultra',\n   'modelName': 'Jurassic-2 Ultra',\n   'providerName': 'AI21 Labs',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': False,\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra-v1',\n   'modelId': 'ai21.j2-ultra-v1',\n   'modelName': 'Jurassic-2 Ultra',\n   'providerName': 'AI21 Labs',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': False,\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1',\n   'modelId': 'anthropic.claude-instant-v1',\n   'modelName': 'Claude Instant',\n   'providerName': 'Anthropic',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': True,\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v1',\n   'modelId': 'anthropic.claude-v1',\n   'modelName': 'Claude',\n   'providerName': 'Anthropic',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': True,\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2',\n   'modelId': 'anthropic.claude-v2',\n   'modelName': 'Claude',\n   'providerName': 'Anthropic',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': True,\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-text-v14',\n   'modelId': 'cohere.command-text-v14',\n   'modelName': 'Command',\n   'providerName': 'Cohere',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': True,\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-light-text-v14',\n   'modelId': 'cohere.command-light-text-v14',\n   'modelName': 'Command Light',\n   'providerName': 'Cohere',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': True,\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-english-v3',\n   'modelId': 'cohere.embed-english-v3',\n   'modelName': 'Embed English',\n   'providerName': 'Cohere',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['EMBEDDING'],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-multilingual-v3',\n   'modelId': 'cohere.embed-multilingual-v3',\n   'modelName': 'Embed Multilingual',\n   'providerName': 'Cohere',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['EMBEDDING'],\n   'inferenceTypesSupported': ['ON_DEMAND']},\n  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1',\n   'modelId': 'meta.llama2-13b-chat-v1',\n   'modelName': 'Llama 2 Chat 13B',\n   'providerName': 'Meta',\n   'inputModalities': ['TEXT'],\n   'outputModalities': ['TEXT'],\n   'responseStreamingSupported': True,\n   'customizationsSupported': [],\n   'inferenceTypesSupported': ['ON_DEMAND']}]}"},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":"bedrock_client = session.client('bedrock')\nbedrock_client.list_foundation_models()"},{"cell_type":"markdown","metadata":{},"source":"### Setup langchain\n\nWe create an instance of the Bedrock classes for the LLM and the embedding models. At the time of writing, Bedrock supports one embedding model and therefore we do not need to specify any model id. To be able to compare token consumption across the different RAG-approaches shown in the workshop labs we use langchain callbacks to count token consumption."},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:35:02.076602Z","iopub.status.busy":"2023-11-13T21:35:02.076216Z","iopub.status.idle":"2023-11-13T21:35:02.462751Z","shell.execute_reply":"2023-11-13T21:35:02.462299Z","shell.execute_reply.started":"2023-11-13T21:35:02.076583Z"},"language":"python","tags":[],"trusted":true},"outputs":[],"source":"bedrock_runtime_client = session.client('bedrock-runtime')\n# We will be using the Titan Embeddings Model to generate our Embeddings.\nfrom langchain.embeddings import BedrockEmbeddings\nfrom langchain.llms.bedrock import Bedrock\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\n# - create the Anthropic Model\nllm = Bedrock(model_id=\"anthropic.claude-v2\", \n              client=bedrock_runtime_client, \n              model_kwargs={\n                  'max_tokens_to_sample': 200\n              }, \n              callbacks=[StreamingStdOutCallbackHandler()])\n\n# - create the Titan Embeddings Model\nbedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",\n                                       client=bedrock_runtime_client)"},{"cell_type":"markdown","metadata":{},"source":"### Data Preparation\nLet's first download some of the files to build our document store.\n\nIn this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on."},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:35:09.848651Z","iopub.status.busy":"2023-11-13T21:35:09.848099Z","iopub.status.idle":"2023-11-13T21:35:11.002453Z","shell.execute_reply":"2023-11-13T21:35:11.001791Z","shell.execute_reply.started":"2023-11-13T21:35:09.848632Z"},"language":"python","tags":[],"trusted":true},"outputs":[],"source":"!mkdir -p ./RAG_Titan_Embeddings_Claude_data\n\nfrom urllib.request import urlretrieve\nurls = [\n    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n]\n\nfilenames = [\n    'AMZN-2022-Shareholder-Letter.pdf',\n    'AMZN-2021-Shareholder-Letter.pdf',\n    'AMZN-2020-Shareholder-Letter.pdf',\n    'AMZN-2019-Shareholder-Letter.pdf'\n]\n\nmetadata = [\n    dict(year=2022, source=filenames[0]),\n    dict(year=2021, source=filenames[1]),\n    dict(year=2020, source=filenames[2]),\n    dict(year=2019, source=filenames[3])]\n\ndata_root = \"./RAG_Titan_Embeddings_Claude_data/\"\n\nfor idx, url in enumerate(urls):\n    file_path = data_root + filenames[idx]\n    urlretrieve(url, file_path)"},{"cell_type":"markdown","metadata":{},"source":"As part of Amazon's culture, the CEO always includes a copy of the 1997 Letter to Shareholders with every new release. This will cause repetition, take longer to generate embeddings, and may skew your results. In the next section you will take the downloaded data, trim the 1997 letter (last 3 pages) and overwrite them as processed files."},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:35:16.757169Z","iopub.status.busy":"2023-11-13T21:35:16.756670Z","iopub.status.idle":"2023-11-13T21:35:16.851568Z","shell.execute_reply":"2023-11-13T21:35:16.851131Z","shell.execute_reply.started":"2023-11-13T21:35:16.757143Z"},"language":"python","tags":[],"trusted":true},"outputs":[],"source":"from pypdf import PdfReader, PdfWriter\nimport glob\n\nlocal_pdfs = glob.glob(data_root + '*.pdf')\n\nfor local_pdf in local_pdfs:\n    pdf_reader = PdfReader(local_pdf)\n    pdf_writer = PdfWriter()\n    for pagenum in range(len(pdf_reader.pages)-3):\n        page = pdf_reader.pages[pagenum]\n        pdf_writer.add_page(page)\n\n    with open(local_pdf, 'wb') as new_file:\n        new_file.seek(0)\n        pdf_writer.write(new_file)\n        new_file.truncate()"},{"cell_type":"markdown","metadata":{},"source":"After downloading we can load the documents with the help of [DirectoryLoader from PyPDF available under LangChain](https://python.langchain.com/en/latest/reference/modules/document_loaders.html) and splitting them into smaller chunks.\n\nNote: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limit of the length of input tokens limited to 512 tokens, which roughly translates to ~2000 characters. For the sake of this use-case we are creating chunks of roughly 1000 characters with an overlap of 100 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)."},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:35:22.648184Z","iopub.status.busy":"2023-11-13T21:35:22.647761Z","iopub.status.idle":"2023-11-13T21:35:23.070521Z","shell.execute_reply":"2023-11-13T21:35:23.070049Z","shell.execute_reply.started":"2023-11-13T21:35:22.648164Z"},"language":"python","tags":[],"trusted":true},"outputs":[],"source":"import numpy as np\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import PyPDFLoader\n\ndocuments = []\n\nfor idx, file in enumerate(filenames):\n    loader = PyPDFLoader(data_root + file)\n    document = loader.load()\n    for document_fragment in document:\n        document_fragment.metadata = metadata[idx]\n        \n    #print(f'{len(document)} {document}\\n')\n    documents += document\n\n# - in our testing Character split works better with this PDF data set\ntext_splitter = RecursiveCharacterTextSplitter(\n    # Set a really small chunk size, just to show.\n    chunk_size = 1000,\n    chunk_overlap  = 100,\n)\n\ndocs = text_splitter.split_documents(documents)"},{"cell_type":"markdown","metadata":{},"source":"Before we are proceeding we are looking into some interesting statistics regarding the document preprocessing we just performed:"},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:35:29.838986Z","iopub.status.busy":"2023-11-13T21:35:29.838584Z","iopub.status.idle":"2023-11-13T21:35:29.842824Z","shell.execute_reply":"2023-11-13T21:35:29.842319Z","shell.execute_reply.started":"2023-11-13T21:35:29.838963Z"},"language":"python","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Average length among 25 documents loaded is 4131 characters.\nAfter the split we have 151 documents as opposed to the original 25.\nAverage length among 151 documents (after split) is 699 characters.\n"}],"source":"avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\nprint(f'Average length among {len(documents)} documents loaded is {avg_doc_length(documents)} characters.')\nprint(f'After the split we have {len(docs)} documents as opposed to the original {len(documents)}.')\nprint(f'Average length among {len(docs)} documents (after split) is {avg_doc_length(docs)} characters.')"},{"cell_type":"markdown","metadata":{},"source":"We had 4 PDF documents which have been split into smaller chunks.\n\nNow we can see how a sample embedding would look like for one of those chunks."},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:35:35.156154Z","iopub.status.busy":"2023-11-13T21:35:35.155904Z","iopub.status.idle":"2023-11-13T21:35:35.288037Z","shell.execute_reply":"2023-11-13T21:35:35.287472Z","shell.execute_reply.started":"2023-11-13T21:35:35.156137Z"},"language":"python","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Sample embedding of a document chunk:  [ 0.71484375 -0.49804688 -0.0014267  ...  0.3125     -0.09130859\n -0.42578125]\nSize of the embedding:  (1536,)\n"}],"source":"sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\nprint(\"Sample embedding of a document chunk: \", sample_embedding)\nprint(\"Size of the embedding: \", sample_embedding.shape)"},{"cell_type":"markdown","metadata":{},"source":"Following the very same approach embeddings can be generated for the entire corpus and stored in a vector store.\n\nThis can be easily done using SingleStoreDB implementation inside [LangChain](https://python.langchain.com/docs/integrations/vectorstores/singlestoredb) which takes  input the embeddings model and the documents to create the entire vector store.\n\n**⚠️⚠️⚠️ NOTE: it might take few minutes to run the following cell ⚠️⚠️⚠️**"},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:35:42.331601Z","iopub.status.busy":"2023-11-13T21:35:42.331245Z","iopub.status.idle":"2023-11-13T21:35:56.661512Z","shell.execute_reply":"2023-11-13T21:35:56.661033Z","shell.execute_reply.started":"2023-11-13T21:35:42.331580Z"},"language":"python","tags":[],"trusted":true},"outputs":[],"source":"from langchain.vectorstores import SingleStoreDB\n\nvectorstore_s2 = SingleStoreDB.from_documents(\n    docs,\n    bedrock_embeddings,\n)"},{"cell_type":"markdown","metadata":{},"source":"### Question Answering\n\nNow that we have our vector store in place, we can start asking questions."},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:36:01.309345Z","iopub.status.busy":"2023-11-13T21:36:01.308979Z","iopub.status.idle":"2023-11-13T21:36:01.311785Z","shell.execute_reply":"2023-11-13T21:36:01.311291Z","shell.execute_reply.started":"2023-11-13T21:36:01.309327Z"},"language":"python","tags":[],"trusted":true},"outputs":[],"source":"query = \"How has AWS evolved?\""},{"cell_type":"markdown","metadata":{},"source":"The first step would be to create an embedding of the query such that it could be compared with the documents"},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:36:05.914746Z","iopub.status.busy":"2023-11-13T21:36:05.914366Z","iopub.status.idle":"2023-11-13T21:36:06.006540Z","shell.execute_reply":"2023-11-13T21:36:06.005989Z","shell.execute_reply.started":"2023-11-13T21:36:05.914724Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/plain":"array([ 0.75390625, -0.34375   , -0.37890625, ...,  0.34765625,\n       -0.734375  , -0.13183594])"},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":"query_embedding = bedrock_embeddings.embed_query(\"This is a content of the document\")\nnp.array(query_embedding)"},{"cell_type":"markdown","metadata":{},"source":"We can use this embedding of the query to then fetch relevant documents.\nNow our query is represented as embeddings we can do a similarity search of our query against our data store providing us with the most relevant information."},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:36:10.583464Z","iopub.status.busy":"2023-11-13T21:36:10.583224Z","iopub.status.idle":"2023-11-13T21:36:10.726350Z","shell.execute_reply":"2023-11-13T21:36:10.725744Z","shell.execute_reply.started":"2023-11-13T21:36:10.583449Z"},"language":"python","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"4 documents are fetched which are relevant to the query.\n----\n## Document 1: a significant differentiator), butalso allowed us to arrive at the much more game-\nchanging offering that AWS is today........\n---\n## Document 2: This growth also created short-term logistics and cost challenges. We spent Amazon’s\nfirst 25 years building.......\n---\n## Document 3: In the early days of AWS, people sometimes asked us why compute wouldn’t just be an\nundifferentiated.......\n---\n## Document 4: it’s been. Whatever role Amazon played in the world up to that point became further\nmagnified as mostphysical venues shut down for long periods of time and people spent their days at\nhome. This meant thathundreds of millions of people relied on Amazon for PPE, food, clothing, and\nvarious other items thathelped them navigate this unprecedented time. Businesses and governments\nalso had to shift, practicallyovernight, from working with colleagues and technology on-premises to\nworking remotely. AWS played amajor role in enabling this business continuity. Whether companies saw\nextraordinary demand spikes, ordemand diminish quickly with reduced external consumption, the\ncloud’s elasticity to scale capacity up anddown quickly, as well as AWS’s unusually broad\nfunctionality helped millions of companies adjust to thesedifficult circumstances.\nOur AWS and Consumer businesses have had different demand trajectories during the pandemic. In\nthe.......\n---\n"}],"source":"relevant_documents = vectorstore_s2.similarity_search(query)\nprint(f'{len(relevant_documents)} documents are fetched which are relevant to the query.')\nprint('----')\nfor i, rel_doc in enumerate(relevant_documents):\n    print_ww(f'## Document {i+1}: {rel_doc.page_content}.......')\n    print('---')"},{"cell_type":"markdown","metadata":{},"source":"Now we have the relevant documents, it's time to use the LLM to generate an answer based on these documents. \n\nWe will take our inital prompt, together with our relevant documents which were retreived based on the results of our similarity search. We then by combining these create a prompt that we feed back to the model to get our result. At this point our model should give us highly informed information on how we can change the tire of our specific car as it was outlined in our manual.\n\nLangChain provides an abstraction of how this can be done easily."},{"cell_type":"markdown","metadata":{},"source":"### Customisable option\nIn the above scenario you explored the quick and easy way to get a context-aware answer to your question. Now let's have a look at a more customizable option with the help of [RetrievalQA](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html) where you can customize how the documents fetched should be added to prompt using `chain_type` parameter. Also, if you want to control how many relevant documents should be retrieved then change the `k` parameter in the cell below to see different outputs. In many scenarios you might want to know which were the source documents that the LLM used to generate the answer, you can get those documents in the output using `return_source_documents` which returns the documents that are added to the context of the LLM prompt. `RetrievalQA` also allows you to provide a custom [prompt template](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html) which can be specific to the model.\n\nNote: In this example we are using Anthropic Claude as the LLM under Amazon Bedrock, this particular model performs best if the inputs are provided under `Human:` and the model is requested to generate an output after `Assistant:`. In the cell below you see an example of how to control the prompt such that the LLM stays grounded and doesn't answer outside the context."},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:36:18.150265Z","iopub.status.busy":"2023-11-13T21:36:18.149904Z","iopub.status.idle":"2023-11-13T21:36:18.532128Z","shell.execute_reply":"2023-11-13T21:36:18.531667Z","shell.execute_reply.started":"2023-11-13T21:36:18.150247Z"},"language":"python","tags":[],"trusted":true},"outputs":[],"source":"from langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\n\nprompt_template = \"\"\"\n\nHuman: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\n\nAssistant:\"\"\"\nPROMPT = PromptTemplate(\n    template=prompt_template, input_variables=[\"context\", \"question\"]\n)\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore_s2.as_retriever(\n        search_type=\"similarity\", search_kwargs={\"k\": 3}\n    ),\n    return_source_documents=True,\n    chain_type_kwargs={\"prompt\": PROMPT},\n    callbacks=[StreamingStdOutCallbackHandler()]\n)"},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:36:23.487401Z","iopub.status.busy":"2023-11-13T21:36:23.487044Z","iopub.status.idle":"2023-11-13T21:36:26.005387Z","shell.execute_reply":"2023-11-13T21:36:26.004681Z","shell.execute_reply.started":"2023-11-13T21:36:23.487383Z"},"language":"python","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":" Based on the provided context, it seems AWS evolved from initially offering basic compute services\nto becoming a more expansive and game-changing offering over time. The context indicates this growth\nalso created some logistics and cost challenges for Amazon.\n\n[Document(page_content='a significant differentiator), butalso allowed us to arrive at the much more game-changing offering that AWS is today.', metadata={'source': 'AMZN-2021-Shareholder-Letter.pdf', 'year': 2021}), Document(page_content='This growth also created short-term logistics and cost challenges. We spent Amazon’s first 25 years building', metadata={'source': 'AMZN-2021-Shareholder-Letter.pdf', 'year': 2021}), Document(page_content='In the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated', metadata={'source': 'AMZN-2021-Shareholder-Letter.pdf', 'year': 2021})]\n"}],"source":"query = \"How did AWS evolve?\"\nresult = qa({\"query\": query})\nprint_ww(result['result'])\n\nprint(f\"\\n{result['source_documents']}\")"},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:36:32.055170Z","iopub.status.busy":"2023-11-13T21:36:32.054907Z","iopub.status.idle":"2023-11-13T21:36:35.434785Z","shell.execute_reply":"2023-11-13T21:36:35.434259Z","shell.execute_reply.started":"2023-11-13T21:36:32.055150Z"},"language":"python","scrolled":true,"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":" Based on the provided context, it seems Amazon's success stems from investing in and building its\necommerce and logistics capabilities over many years. The context indicates Amazon grew steadily\nover 25 years by building out its ecommerce platform and logistics networks, and it continues making\ninvestments that leverage these core strengths. However, without more complete information I cannot\nconfidently explain the full reasons behind Amazon's success.\n\n[Document(page_content='This growth also created short-term logistics and cost challenges. We spent Amazon’s first 25 years building', metadata={'source': 'AMZN-2021-Shareholder-Letter.pdf', 'year': 2021}), Document(page_content='Amazon Business is another example of an investment where our ecommerce and logistics capabilities', metadata={'source': 'AMZN-2022-Shareholder-Letter.pdf', 'year': 2022}), Document(page_content='We want to improve workers’ lives beyond pay. Amazon provides every full-time employee with health', metadata={'source': 'AMZN-2019-Shareholder-Letter.pdf', 'year': 2019})]\n"}],"source":"query = \"Why is Amazon successful?\"\nresult = qa({\"query\": query})\nprint_ww(result['result'])\n\nprint(f\"\\n{result['source_documents']}\")"},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:36:40.101688Z","iopub.status.busy":"2023-11-13T21:36:40.101324Z","iopub.status.idle":"2023-11-13T21:36:45.495776Z","shell.execute_reply":"2023-11-13T21:36:45.495239Z","shell.execute_reply.started":"2023-11-13T21:36:40.101670Z"},"language":"python","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":" Based on the context provided, some of the business challenges Amazon has experienced are:\n\n- Short-term logistics and cost challenges from rapid growth of the company in its early years.\n\n- Rising costs to fulfill and deliver orders from Amazon's warehouses and stores (its \"Stores\nfulfillment network\").\n\n- Rapid growth in its consumer business during the COVID-19 pandemic which likely strained its\nfulfillment and delivery capacity.\n\n[Document(page_content='This growth also created short-term logistics and cost challenges. We spent Amazon’s first 25 years building', metadata={'source': 'AMZN-2021-Shareholder-Letter.pdf', 'year': 2021}), Document(page_content='Amazon Business is another example of an investment where our ecommerce and logistics capabilities', metadata={'source': 'AMZN-2022-Shareholder-Letter.pdf', 'year': 2022}), Document(page_content='A critical challenge we’ve continued to tackle is the rising cost to serve in our Stores fulfillment network (i.e.\\nthe cost to get a product from Amazon to a customer)—and we’ve made several changes that we believe will\\nmeaningfully improve our fulfillment costs and speed of delivery .\\nDuring the early part of the pandemic, with many physical stores shut down, our consumer business grew', metadata={'source': 'AMZN-2022-Shareholder-Letter.pdf', 'year': 2022})]\n"}],"source":"query = \"What business challenges has Amazon experienced?\"\nresult = qa({\"query\": query})\nprint_ww(result['result'])\n\nprint(f\"\\n{result['source_documents']}\")"},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:36:50.821881Z","iopub.status.busy":"2023-11-13T21:36:50.821521Z","iopub.status.idle":"2023-11-13T21:36:57.022977Z","shell.execute_reply":"2023-11-13T21:36:57.022338Z","shell.execute_reply.started":"2023-11-13T21:36:50.821862Z"},"language":"python","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":" Based on the context provided, it seems Amazon was impacted by COVID-19 in the following ways:\n\n- There was a major increase in demand for Amazon's retail business as people stayed home and relied\non Amazon for supplies, increasing the company's reach and importance.\n\n- AWS saw increased demand as businesses rapidly shifted to remote work and needed cloud\ninfrastructure. AWS helped many companies adjust and maintain business continuity.\n\n- The growth created logistics and cost challenges for Amazon in the short-term as they worked to\nmeet increased demand.\n\n- The different trajectories of the retail business and AWS underscored the breadth of Amazon's\nbusiness.\n\nIn summary, COVID-19 led to surges in demand that demonstrated the critical role Amazon plays for\nconsumers and businesses, while also surfacing operational challenges as the company scaled to meet\nthese new demands.\n\n[Document(page_content='it’s been. Whatever role Amazon played in the world up to that point became further magnified as mostphysical venues shut down for long periods of time and people spent their days at home. This meant thathundreds of millions of people relied on Amazon for PPE, food, clothing, and various other items thathelped them navigate this unprecedented time. Businesses and governments also had to shift, practicallyovernight, from working with colleagues and technology on-premises to working remotely. AWS played amajor role in enabling this business continuity. Whether companies saw extraordinary demand spikes, ordemand diminish quickly with reduced external consumption, the cloud’s elasticity to scale capacity up anddown quickly, as well as AWS’s unusually broad functionality helped millions of companies adjust to thesedifficult circumstances.\\nOur AWS and Consumer businesses have had different demand trajectories during the pandemic. In the', metadata={'source': 'AMZN-2021-Shareholder-Letter.pdf', 'year': 2021}), Document(page_content='This growth also created short-term logistics and cost challenges. We spent Amazon’s first 25 years building', metadata={'source': 'AMZN-2021-Shareholder-Letter.pdf', 'year': 2021}), Document(page_content='We want to improve workers’ lives beyond pay. Amazon provides every full-time employee with health', metadata={'source': 'AMZN-2019-Shareholder-Letter.pdf', 'year': 2019})]\n"}],"source":"query = \"How was Amazon impacted by COVID-19?\"\n\nresult = qa({\"query\": query})\n\nprint_ww(result['result'])\n\nprint(f\"\\n{result['source_documents']}\")"},{"cell_type":"markdown","metadata":{"language":"python"},"source":"## Clean up\nClear the downloaded PDFs"},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T21:37:10.404092Z","iopub.status.busy":"2023-11-13T21:37:10.403843Z","iopub.status.idle":"2023-11-13T21:37:10.609739Z","shell.execute_reply":"2023-11-13T21:37:10.606270Z","shell.execute_reply.started":"2023-11-13T21:37:10.404075Z"},"language":"python","trusted":true},"outputs":[],"source":"!rm -rf ./RAG_Titan_Embeddings_Claude_data"},{"cell_type":"markdown","metadata":{},"source":"## Conclusion\nRetrieval augmented generation is an important technique that combines the power of large language models with the precision of retrieval methods. By augmenting generation with relevant retrieved examples, the responses we recieved become more coherent, consistent and grounded. You should feel proud of learning this innovative approach. I'm sure the knowledge you've gained will be very useful for building creative and engaging language generation systems. \n\nIn the above implementation of RAG based Question Answering we have explored the following concepts and how to implement them using Amazon Bedrock and it's LangChain integration.\n\n- Loading documents of different kind and generating embeddings to create a vector store\n- Retrieving documents to the question\n- Preparing a prompt which goes as input to the LLM\n- Present an answer in a human friendly manner\n\n### Take-aways\n- Leverage various models available under Amazon Bedrock to see alternate outputs\n- Explore options such as persistent storage of embeddings and document chunks\n- Integration with enterprise data stores\n\n# Thank You"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"singlestore_connection":{"connectionID":"8baad8cd-542b-4097-a35f-b2efc14a8b2e","defaultDatabase":"bedrock_demo"}},"nbformat":4,"nbformat_minor":4}