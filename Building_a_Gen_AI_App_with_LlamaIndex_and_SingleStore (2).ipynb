{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Generative AI Application with LlamaIndex and SingleStore\n",
        "\n",
        "Welcome to this in-depth guide on constructing a Generative AI application utilizing LlamaIndex and SingleStoreDB. This guide will provide a step-by-step walkthrough, code explanations, and best practices.\n",
        "\n",
        "## Overview\n",
        "LlamaIndex is a library dedicated to ingesting, indexing, and querying contextual information for Retrieval Augmented Generation (RAG). In synergy with SingleStoreDB, a scalable and SQL-compliant relational database system, it lays the foundation for building powerful generative AI applications. This combination facilitates real-time data processing and retrieval, essential for answering user queries efficiently. LlamaIndex is also cross compatible with Langchain, another popular library used for composing LLM inputs and outputs. We'll use both with SingleStore to build an end-to-end GenAI app.\n",
        "\n",
        "## What You'll Learn\n",
        "- Setting up the environment with the required packages and credentials.\n",
        "- Ingesting and indexing data using LlamaIndex for efficient retrieval.\n",
        "- Storing and managing data in SingleStoreDB.\n",
        "- Building a retrieval-based generative AI system to respond to user queries.\n",
        "\n",
        "## Prerequisites\n",
        "- Basic knowledge of Python programming.\n",
        "- Understanding of SQL databases.\n",
        "- Familiarity with generative AI concepts would be beneficial.\n"
      ],
      "metadata": {
        "id": "n42ScOAx0IAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first install the necessary packages."
      ],
      "metadata": {
        "id": "8CuScKOz2V--"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8gbadBPcVdq"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install llama-hub --quiet\n",
        "!pip install singlestoredb --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's set our OpenAI API Key."
      ],
      "metadata": {
        "id": "UTyaBjaC2l4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "9nIA0T2Pcbgv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll import the SingleStore vectorstore from Langchain."
      ],
      "metadata": {
        "id": "wGZQKeo02qiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import SingleStoreDB"
      ],
      "metadata": {
        "id": "QgIcQgDOccgj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After importing SingleStore, we can ingest the docs for LlamaIndex into a new table. This takes three steps:\n",
        "\n",
        "1. Load raw HTML data using WebBaseLoader\n",
        "2. Chunk the text.\n",
        "3. Embed or vectorize the chunked text, then ingest it into SingleStore."
      ],
      "metadata": {
        "id": "sUlmrnw721oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://gpt-index.readthedocs.io/en/latest/\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "i5ESqCtVcdv_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
        "all_splits = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "hGBKa8m9cf37"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "os.environ[\"SINGLESTOREDB_URL\"] = \"admin:dWdWT75VFi6iZHwguHptkgGgurjDF7uq@svc-56441794-b2ba-46ad-bc0b-c3d5810a45f4-dml.aws-oregon-3.svc.singlestore.com:3306/demo\"\n",
        "\n",
        "# vectorstore = SingleStoreDB.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
        "vectorstore = SingleStoreDB(embedding=OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "_OQgOifPc4fp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll use Llama Index to retrieve and query from SingleStore using the SingleStoreReader, a lightweight embedding lookup tool for SingleStore databases ingested with content and vector data.\n",
        "\n",
        "Note that the full SingleStore vectorstore integration with Llama Index for ingesting and indexing is coming soon!"
      ],
      "metadata": {
        "id": "_zX7dL2r3UNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import download_loader\n",
        "\n",
        "SingleStoreReader = download_loader(\"SingleStoreReader\")\n",
        "\n",
        "reader = SingleStoreReader(\n",
        "    scheme=\"mysql\",\n",
        "    host=\"svc-56441794-b2ba-46ad-bc0b-c3d5810a45f4-dml.aws-oregon-3.svc.singlestore.com\",\n",
        "    port=\"3306\",\n",
        "    user=\"admin\",\n",
        "    password=\"dWdWT75VFi6iZHwguHptkgGgurjDF7uq\",\n",
        "    dbname=\"demo\",\n",
        "    table_name=\"embeddings\",\n",
        "    content_field=\"content\",\n",
        "    vector_field=\"vector\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQy1IgKvhUB4",
        "outputId": "26109ce0-5974-46b5-e025-e4d9dedd8bc8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/utilities/sql_database.py:112: SAWarning: Unknown schema content: '  SORT KEY `__UNORDERED` ()'\n",
            "  self._metadata.reflect(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/utilities/sql_database.py:112: SAWarning: Unknown schema content: '  , SHARD KEY () '\n",
            "  self._metadata.reflect(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out. This function takes a natural language query as input, then does the following:\n",
        "\n",
        "1. Embed the query using the OpenAI Embedding model, `text-embedding-ada-002` by default.\n",
        "2. Ingest the documents into a Llama Index list index, a data structure that returns all documents into the context.\n",
        "3. Initialize the index as a Llama Index query engine, which uses the `gpt-3.5-turbo` OpenAI LLM by default to understand the query and provided context, then generate a response.\n",
        "4. Returns the response."
      ],
      "metadata": {
        "id": "ejV-Yn3R4AwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "from llama_index import ListIndex\n",
        "\n",
        "def ask_llamaindex_docs(query):\n",
        "\n",
        "  embeddings = OpenAIEmbeddings()\n",
        "  search_embedding = embeddings.embed_query(query)\n",
        "  documents = reader.load_data(search_embedding=json.dumps(str(search_embedding)))\n",
        "\n",
        "  index = ListIndex(documents)\n",
        "\n",
        "  query_engine = index.as_query_engine()\n",
        "\n",
        "  response = query_engine.query(query)\n",
        "  return response"
      ],
      "metadata": {
        "id": "PQ9earK8yPzB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask_llamaindex_docs(\"What is Llama Index?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtFluQttpk7a",
        "outputId": "d15c3225-1a09-47b8-a531-8c9c6db7e61c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama Index is a data framework for LLM applications to ingest, structure, and access private or domain-specific data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask_llamaindex_docs(\"What are data indexes in Llama Index?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKtIf_lC1bcw",
        "outputId": "b6ca291f-92fb-4cd2-b849-229eed7ba7ce"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data indexes in Llama Index are modules that allow users to organize and retrieve their data efficiently. These indexes can be customized and extended to fit the specific needs of the users.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask_llamaindex_docs(\"What are query engines in Llama Index?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs9btjMB10If",
        "outputId": "95314fc2-35e8-4c5c-bc25-cdc71b018edc"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query engines in Llama Index are components that handle different types of queries on the data stored in the index. They include the Graph Query Engine, Multistep Query Engine, Retriever Query Engine, Transform Query Engine, Router Query Engine, Retriever Router Query Engine, Sub Question Query Engine, SQL Join Query Engine, Flare Query Engine, Citation Query Engine, Knowledge Graph Query Engine, SQL Query Engine, and Pandas Query Engine. Each query engine is designed to handle specific types of queries and provide efficient and accurate results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tips and Tricks\n",
        "\n",
        "## Chat engines\n",
        "\n",
        "Chat with your data conversationally with Llama Index Chat Engines, which allow for follow-ups and further questions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PT4HR_t36sNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Llama Index?\"\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "search_embedding = embeddings.embed_query(query)\n",
        "documents = reader.load_data(search_embedding=json.dumps(str(search_embedding)))\n",
        "\n",
        "index = ListIndex(documents)\n",
        "\n",
        "chat_engine = index.as_chat_engine(chat_mode='context')\n",
        "\n",
        "chat_engine.chat_repl()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "rmygl6G67WYw",
        "outputId": "76ec0289-477a-4e6d-d8c4-00088099f804"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Entering Chat REPL =====\n",
            "Type \"exit\" to exit.\n",
            "\n",
            "Human: What is Llama Index?\n",
            "Assistant: LlamaIndex is an open-source library that provides tools and APIs for building and deploying large-scale search and retrieval systems. It allows users to ingest, index, and query large amounts of data efficiently. LlamaIndex supports various types of data stores, including document stores, key-value stores, and graph stores. It also provides different indexing techniques and retrieval models to optimize search performance. LlamaIndex is designed to be flexible and customizable, allowing users to tailor the system to their specific needs. It is suitable for both beginners and advanced users, offering a high-level API for quick and easy usage, as well as lower-level APIs for customization and extension.\n",
            "\n",
            "Human: What can Llama Index do?\n",
            "Assistant: LlamaIndex offers a range of capabilities for building and deploying search and retrieval systems. Here are some key features and functionalities of LlamaIndex:\n",
            "\n",
            "1. Data Ingestion: LlamaIndex provides tools for ingesting large amounts of data from various sources, including document stores, key-value stores, and graph stores.\n",
            "\n",
            "2. Indexing: LlamaIndex supports different indexing techniques, such as inverted indexes, forward indexes, and graph indexes. These indexes enable efficient and fast retrieval of relevant information.\n",
            "\n",
            "3. Querying: LlamaIndex allows users to perform complex queries on the indexed data. It supports various query types, including keyword search, phrase search, fuzzy search, and advanced search operators.\n",
            "\n",
            "4. Customization: LlamaIndex offers a modular architecture that allows users to customize and extend different components of the system. This includes data connectors, indices, retrievers, query engines, and reranking modules.\n",
            "\n",
            "5. Scalability: LlamaIndex is designed to handle large-scale data and can scale horizontally to accommodate growing data volumes and user demands.\n",
            "\n",
            "6. Performance Optimization: LlamaIndex provides optimization techniques to improve search performance, such as query rewriting, caching, and result ranking strategies.\n",
            "\n",
            "7. Evaluation: LlamaIndex includes tools for evaluating the performance of search and retrieval systems. It offers modules for measuring relevancy, correctness, embedding similarity, and other evaluation metrics.\n",
            "\n",
            "8. Integration: LlamaIndex can be integrated with other tools and frameworks, such as deep learning models, knowledge graph query engines, and language chains, to enhance the capabilities of the system.\n",
            "\n",
            "Overall, LlamaIndex empowers users to build efficient and customizable search and retrieval systems, making it suitable for a wide range of applications and use cases.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-94e987d92ce5>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mchat_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_chat_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mchat_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_repl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/chat_engine/types.py\u001b[0m in \u001b[0;36mchat_repl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Assistant: {response}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Human: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune Embeddings\n",
        "\n",
        "Improve your retrieval performance by 5-10% using a finetuned embedding model. Though a full implementation is outside the scope of this webinar, at a high level, you will:\n",
        "\n",
        "1. Split your data into train and validation datasets.\n",
        "2. Generate synthetic QA embedding pairs.\n",
        "3. Finetune your model using Llama Index.\n"
      ],
      "metadata": {
        "id": "t3BIgvoC8o6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your data goes here\n",
        "train_dataset = generate_qa_embedding_pairs(train_nodes)\n",
        "val_dataset = generate_qa_embedding_pairs(val_nodes)"
      ],
      "metadata": {
        "id": "pf_3UIth_B0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.finetuning import SentenceTransformersFinetuneEngine"
      ],
      "metadata": {
        "id": "FqZnxwey-qS0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_engine = SentenceTransformersFinetuneEngine(\n",
        "    train_dataset,\n",
        "    model_id=\"BAAI/bge-small-en\",\n",
        "    model_output_path=\"test_model\",\n",
        "    val_dataset=val_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "KgdmYCFh-q9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_engine.finetune()"
      ],
      "metadata": {
        "id": "JwAsIF9s_GJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = finetune_engine.get_finetuned_model()"
      ],
      "metadata": {
        "id": "hrlj9KSn_Hnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "EQ-A880W6iLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Agents\n",
        "\n",
        "Data Agents are agents in LlamaIndex that can reason over your data and perform predefined tasks, with the ability to read and modify your data. They can:\n",
        "\n",
        "- Perform automated search and retrieval over different types of data - unstructured, semi-structured, and structured.\n",
        "\n",
        "- Calling any external service API in a structured fashion, and processing the response + storing it for later.\n",
        "\n",
        "We'll create a simple agent with access to the `ask_llamaindex_docs` function we created earlier as a tool."
      ],
      "metadata": {
        "id": "6KW64Bkm_PXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import OpenAI\n",
        "from llama_index.agent import ReActAgent\n",
        "from llama_index.tools import QueryEngineTool, ToolMetadata"
      ],
      "metadata": {
        "id": "mhKFkh6JAFYS"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llamaindex_docs_tool = QueryEngineTool(\n",
        "    query_engine=index.as_query_engine(),\n",
        "    metadata=ToolMetadata(\n",
        "        name=\"llamaindex_docs\",\n",
        "        description=\"Provides access to the docs for Llama Index, a library for ingesting, indexing, and querying data for LLMs.\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "-oPT5nm0AGON"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = ReActAgent.from_tools([llamaindex_docs_tool], verbose=True)"
      ],
      "metadata": {
        "id": "s_9E-vfBA3Od"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.reset()"
      ],
      "metadata": {
        "id": "1MDc-YQaBMZH"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.chat(\"What is Llama Index?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIBOT2ySA8sr",
        "outputId": "271cd471-49c2-4760-f00f-0188bf13b7af"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;200m\u001b[1;3mThought: I need to use a tool to help me answer the question.\n",
            "Action: llamaindex_docs\n",
            "Action Input: {'input': 'Llama Index'}\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mObservation: LlamaIndex is a tool that provides APIs for both beginner and advanced users. It allows beginners to easily ingest and query their data with just a few lines of code. Advanced users can customize and extend various modules, such as data connectors, indices, retrievers, query engines, and reranking modules, to suit their specific needs. LlamaIndex also supports different types of stores, including document stores, index stores, key-value stores, and graph stores. It offers various tutorials and guides to help users understand and utilize its features effectively.\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mResponse: Llama Index is a tool that provides APIs for both beginner and advanced users. It allows beginners to easily ingest and query their data with just a few lines of code. Advanced users can customize and extend various modules, such as data connectors, indices, retrievers, query engines, and reranking modules, to suit their specific needs. Llama Index also supports different types of stores, including document stores, index stores, key-value stores, and graph stores. It offers various tutorials and guides to help users understand and utilize its features effectively.\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentChatResponse(response='Llama Index is a tool that provides APIs for both beginner and advanced users. It allows beginners to easily ingest and query their data with just a few lines of code. Advanced users can customize and extend various modules, such as data connectors, indices, retrievers, query engines, and reranking modules, to suit their specific needs. Llama Index also supports different types of stores, including document stores, index stores, key-value stores, and graph stores. It offers various tutorials and guides to help users understand and utilize its features effectively.', sources=[], source_nodes=[])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.chat(\"Tell me about it's capabiltiies\")"
      ],
      "metadata": {
        "id": "URn-hFCNBBL7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}