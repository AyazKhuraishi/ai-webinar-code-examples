{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"0etRtS83RcWS"},"source":"# SingleStore and Groq RAG Quickstart"},{"attachments":{},"cell_type":"markdown","metadata":{"id":"r1IzNLho-NqV"},"source":"This notebook provides an example of how to use SingleStore as a vector database in conjunction with Groq, the world's fastest LLM."},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T07:03:54.919609Z","iopub.status.busy":"2024-05-02T07:03:54.919230Z","iopub.status.idle":"2024-05-02T07:04:09.803928Z","shell.execute_reply":"2024-05-02T07:04:09.791624Z","shell.execute_reply.started":"2024-05-02T07:03:54.919573Z"},"language":"python","trusted":true},"outputs":[],"source":"!pip install -q -U langchain langchain-groq singlestoredb langchain-openai --quiet"},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T07:04:45.525503Z","iopub.status.busy":"2024-05-02T07:04:45.525125Z","iopub.status.idle":"2024-05-02T07:05:04.099932Z","shell.execute_reply":"2024-05-02T07:05:04.098886Z","shell.execute_reply.started":"2024-05-02T07:04:45.525474Z"},"language":"python","trusted":true},"outputs":[{"name":"stdin","output_type":"stream","text":" ········\n"}],"source":"from getpass import getpass\n\nimport os\n\nGROQ_API_KEY = getpass()\n\nos.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T07:07:30.774357Z","iopub.status.busy":"2024-05-02T07:07:30.773770Z","iopub.status.idle":"2024-05-02T07:07:35.700618Z","shell.execute_reply":"2024-05-02T07:07:35.700002Z","shell.execute_reply.started":"2024-05-02T07:07:30.774327Z"},"language":"python","trusted":true},"outputs":[{"name":"stdin","output_type":"stream","text":" ········\n"}],"source":"OPENAI_API_KEY = getpass()\n\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T07:07:01.938444Z","iopub.status.busy":"2024-05-02T07:07:01.938072Z","iopub.status.idle":"2024-05-02T07:07:02.500894Z","shell.execute_reply":"2024-05-02T07:07:02.500153Z","shell.execute_reply.started":"2024-05-02T07:07:01.938411Z"},"language":"python","trusted":true},"outputs":[],"source":"from langchain_groq import ChatGroq\nfrom langchain_core.prompts import ChatPromptTemplate"},{"attachments":{},"cell_type":"markdown","metadata":{"id":"m01XDoo4UQvN"},"source":"## Initialize GroqChat"},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T07:13:12.578759Z","iopub.status.busy":"2024-05-02T07:13:12.578301Z","iopub.status.idle":"2024-05-02T07:13:12.616178Z","shell.execute_reply":"2024-05-02T07:13:12.615557Z","shell.execute_reply.started":"2024-05-02T07:13:12.578720Z"},"language":"python","trusted":true},"outputs":[],"source":"groq = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")"},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T07:11:40.087810Z","iopub.status.busy":"2024-05-02T07:11:40.087468Z","iopub.status.idle":"2024-05-02T07:11:40.120633Z","shell.execute_reply":"2024-05-02T07:11:40.119896Z","shell.execute_reply.started":"2024-05-02T07:11:40.087785Z"},"language":"python","trusted":true},"outputs":[],"source":"from langchain_openai import ChatOpenAI\n\nopenai = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"},{"cell_type":"markdown","metadata":{"language":"python"},"source":"# Now let's test groq!"},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T07:14:56.734805Z","iopub.status.busy":"2024-05-02T07:14:56.734397Z","iopub.status.idle":"2024-05-02T07:14:57.739193Z","shell.execute_reply":"2024-05-02T07:14:57.738711Z","shell.execute_reply.started":"2024-05-02T07:14:56.734767Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/plain":"AIMessage(content=\"Large Language Models (LLMs) have revolutionized the field of natural language processing (NLP) by enabling applications such as language translation, text summarization, and chatbots. However, traditional LLMs often suffer from high latency, which can be a significant limitation in many applications. Low latency LLMs, on the other hand, offer several advantages that make them crucial for various use cases. Here are some reasons why low latency LLMs are important:\\n\\n1. **Real-time applications**: In applications like chatbots, virtual assistants, and real-time language translation, low latency is essential to provide a seamless user experience. Low latency LLMs enable faster response times, reducing the delay between user input and the AI's response.\\n2. **Interactive systems**: Interactive systems like language-based games, quizzes, or educational platforms require low latency to ensure a smooth user experience. Low latency LLMs can process user input quickly, providing instant feedback and maintaining user engagement.\\n3. **Real-time analytics and insights**: In industries like finance, healthcare, or customer service, low latency LLMs can analyze large amounts of data in real-time, providing valuable insights and enabling data-driven decision-making.\\n4. **Edge computing and IoT**: With the increasing adoption of edge computing and IoT devices, low latency LLMs can process data locally, reducing latency and improving response times in applications like smart homes, industrial automation, or autonomous vehicles.\\n5. **Improved user experience**: Low latency LLMs can provide a more responsive and engaging user experience, reducing frustration and increasing user satisfaction in applications like language translation, text summarization, or chatbots.\\n6. **Competitive advantage**: In competitive industries like customer service, finance, or e-commerce, low latency LLMs can provide a competitive advantage by enabling faster response times, improved customer satisfaction, and increased conversions.\\n7. **Scalability and efficiency**: Low latency LLMs can be designed to scale more efficiently, reducing the computational resources required to process large amounts of data, which is particularly important in cloud-based or distributed computing environments.\\n8. **Enhanced security**: Low latency LLMs can be designed with enhanced security features, such as encryption and secure communication protocols, to protect sensitive data and prevent unauthorized access.\\n\\nIn summary, low latency LLMs are crucial for applications that require real-time processing, interactive systems, and real-time analytics. They offer a competitive advantage, improved user experience, and enhanced security, making them essential for various industries and use cases.\", response_metadata={'token_usage': {'completion_time': 0.613, 'completion_tokens': 508, 'prompt_time': 0.014, 'prompt_tokens': 32, 'queue_time': None, 'total_time': 0.627, 'total_tokens': 540}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_af05557ca2', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a0eb815-113c-4981-a979-9a60c5745cdb-0')"},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":"system = \"You are a helpful assistant.\"\nhuman = \"{text}\"\nprompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n\nchain = prompt | groq\nchain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"},{"cell_type":"code","execution_count":null,"metadata":{"language":"python","trusted":true},"outputs":[],"source":"# OpenAI GPT 3.5 Turbo for comparison"},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T07:14:47.035597Z","iopub.status.busy":"2024-05-02T07:14:47.034830Z","iopub.status.idle":"2024-05-02T07:14:52.763091Z","shell.execute_reply":"2024-05-02T07:14:52.762095Z","shell.execute_reply.started":"2024-05-02T07:14:47.035561Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/plain":"AIMessage(content='Low latency LLMs, or Low-Latency Memory Modules, are important in high-performance computing environments where fast data access is critical. Here are some reasons why low latency LLMs are important:\\n\\n1. Reduced response time: Low latency LLMs provide faster access to data, reducing the time it takes for the CPU to retrieve information from memory. This results in quicker response times for applications, leading to improved overall system performance.\\n\\n2. Enhances system efficiency: By minimizing latency, low latency LLMs help in reducing data access bottlenecks and increasing the efficiency of data processing. This is particularly important in real-time applications where timely data retrieval is crucial.\\n\\n3. Increased throughput: Low latency LLMs can improve the overall throughput of a system by allowing data to be accessed and processed more quickly. This can be beneficial for applications that require high data transfer rates, such as in financial trading or scientific research.\\n\\n4. Better user experience: In consumer applications, low latency LLMs can improve the user experience by reducing loading times and improving the responsiveness of applications. This can lead to higher customer satisfaction and retention.\\n\\nOverall, low latency LLMs play a crucial role in enhancing system performance, improving data access speeds, and providing a better user experience in various computing environments.', response_metadata={'token_usage': {'completion_tokens': 259, 'prompt_tokens': 28, 'total_tokens': 287}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_a450710239', 'finish_reason': 'stop', 'logprobs': None}, id='run-aa35104e-8007-4a7b-9e7e-9319397bfc1e-0')"},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":"system = \"You are a helpful assistant.\"\nhuman = \"{text}\"\nprompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n\nchain = prompt | openai\nchain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"},{"attachments":{},"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-04-26T08:44:11.362703Z","iopub.status.busy":"2024-04-26T08:44:11.362215Z","iopub.status.idle":"2024-04-26T08:44:11.364950Z","shell.execute_reply":"2024-04-26T08:44:11.364515Z","shell.execute_reply.started":"2024-04-26T08:44:11.362679Z"},"language":"python"},"source":"## RAG using SingleStoreDB"},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T07:15:24.937182Z","iopub.status.busy":"2024-05-02T07:15:24.936798Z","iopub.status.idle":"2024-05-02T07:15:24.941820Z","shell.execute_reply":"2024-05-02T07:15:24.941153Z","shell.execute_reply.started":"2024-05-02T07:15:24.937156Z"},"language":"python","trusted":true},"outputs":[],"source":"from langchain.vectorstores import SingleStoreDB\nimport os\n\nfrom langchain_openai import OpenAIEmbeddings\n\nos.environ[\"SINGLESTOREDB_URL\"] = f'{connection_user}:{connection_password}@{connection_host}:{connection_port}/{connection_default_database}'"},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T07:15:23.102499Z","iopub.status.busy":"2024-05-02T07:15:23.102071Z","iopub.status.idle":"2024-05-02T07:15:23.362855Z","shell.execute_reply":"2024-05-02T07:15:23.362094Z","shell.execute_reply.started":"2024-05-02T07:15:23.102460Z"},"language":"python","trusted":true},"outputs":[],"source":"from langchain.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://python.langchain.com/docs/integrations/chat/groq/\")\ndata = loader.load()"},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T07:15:25.943232Z","iopub.status.busy":"2024-05-02T07:15:25.942826Z","iopub.status.idle":"2024-05-02T07:15:25.949098Z","shell.execute_reply":"2024-05-02T07:15:25.948374Z","shell.execute_reply.started":"2024-05-02T07:15:25.943198Z"},"language":"python","trusted":true},"outputs":[],"source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\nall_splits = text_splitter.split_documents(data)"},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T07:15:33.693898Z","iopub.status.busy":"2024-05-02T07:15:33.693518Z","iopub.status.idle":"2024-05-02T07:15:36.589102Z","shell.execute_reply":"2024-05-02T07:15:36.588349Z","shell.execute_reply.started":"2024-05-02T07:15:33.693869Z"},"language":"python","trusted":true},"outputs":[],"source":"vectorstore=SingleStoreDB.from_documents(documents=all_splits, table_name=\"test9\", embedding=OpenAIEmbeddings())"},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2024-05-02T07:15:55.771195Z","iopub.status.busy":"2024-05-02T07:15:55.770797Z","iopub.status.idle":"2024-05-02T07:15:57.433926Z","shell.execute_reply":"2024-05-02T07:15:57.433268Z","shell.execute_reply.started":"2024-05-02T07:15:55.771161Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/plain":"{'query': 'Please show a simple example of how to chat with Groq with Langchain in python.',\n 'result': 'Here is a simple example of how to chat with Groq using Langchain in Python:\\n\\n```\\nfrom langchain.groq import ChatGroq\\nfrom langchain.prompts import ChatPromptTemplate\\n\\n# Initialize the ChatGroq class\\nchat = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\\n\\n# Define the system message\\nsystem = \"You are a helpful assistant.\"\\n\\n# Define the human message\\nhuman = \"What is the definition of Groq?\"\\n\\n# Create a prompt template\\nprompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\\n\\n# Invoke ChatGroq to create completions\\nchain = prompt | chat.invoke({\"text\": \"Explain the definition of Groq.\"})\\n\\n# Print the response\\nprint(chain)\\n```\\n\\nThis code initializes the `ChatGroq` class with a temperature of 0 and a model name of \"mixtral-8x7b-32768\". It then defines a system message and a human message, creates a prompt template using these messages, and invokes the `ChatGroq` class to generate a response to the human message. Finally, it prints the response.'}"},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":"from langchain.chains import RetrievalQA\n\nqa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever())\nqa_chain({\"query\": \"Please show a simple example of how to chat with Groq with Langchain in python.\"})"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"singlestore_cell_default_language":"python","singlestore_connection":{"connectionID":"6efce35a-2db5-4ae3-bc0b-c3d5810a45f4","defaultDatabase":"llama3_demo"}},"nbformat":4,"nbformat_minor":4}