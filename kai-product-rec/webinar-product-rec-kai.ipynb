{"cells":[{"cell_type":"markdown","id":"d977a96a-791b-4e4f-9690-51b4d79112eb","metadata":{},"source":"# Using GPT for Product Recommendation Engines\n\nToday we will be using SingleStore Kaiâ„¢ for MongoDB, along with OpenAI, to put together a simple product recommendation engine in Python."},{"cell_type":"markdown","id":"854151ef-f222-4d65-a07e-391e4aae7b7d","metadata":{},"source":"## API Keys\n\nBefore you get started, make sure you drop in your API key for OPENAI and your OPENAI ORG.\n\n- You can find your `OPENAI_API_KEY` [here](https://platform.openai.com/account/api-keys), if you don't have one, generate a project specific one.\n- You can find your `OPENAI_ORG` [here](https://platform.openai.com/account/org-settings) under `Organization ID`."},{"cell_type":"code","execution_count":null,"id":"db397f6b-15cf-451b-b52c-ba82e6d08764","metadata":{"tags":[],"trusted":true},"outputs":[],"source":"OPENAI_API_KEY = ''\nOPENAI_ORG = ''"},{"cell_type":"markdown","id":"4c6998dc-9a1c-452d-8bba-4b631ec189da","metadata":{},"source":"## Import Dataset\n\nFirst, we'll import a dataset into our database. Let's use [Open Library's Works](https://openlibrary.org/data/ol_dump_works_latest.txt.gz) dataset.\n\nFor the sake of brevity for the webinar, we've downloaded the extremely large dataset linked above and restricted it to just the Science Fiction novels using the following commands:"},{"cell_type":"code","execution_count":null,"id":"a2121dbe-2c4e-42d7-b02c-596a1de720db","metadata":{},"outputs":[],"source":"wget https://openlibrary.org/data/ol_dump_works_latest.txt.gz\ngunzip ol_dump_works_latest.txt\ncat ol_dump_works_latest.txt | grep -i \"science fiction\" | grep -i \"description\" | cut -f5 \u003e books_scifi.txt"},{"cell_type":"markdown","id":"1f5897b3-14c6-46fc-981e-ea07c99d2b00","metadata":{},"source":"#### Example Text Line"},{"cell_type":"code","execution_count":null,"id":"0ac45639-895c-47c7-9304-5d54b15e6899","metadata":{},"outputs":[],"source":"{\n    \"created\": {\n        \"type\": \"/type/datetime\",\n        \"value\": \"2009-12-11T01:41:04.053897\"\n    },\n    \"latest_revision\": 4,\n    \"description\": \"A science fiction suspense story. The setting starts on Earth, but then branches off to other realms with interesting\\r\\ncharacters.\",\n    \"key\": \"/works/OL9889262W\",\n    \"title\": \"Dark Paladin\",\n    \"authors\": [\n        {\n            \"type\": {\n                \"key\": \"/type/author_role\"\n            },\n            \"author\": {\n                \"key\": \"/authors/OL3871508A\"\n            }\n        }\n    ],\n    \"type\": {\n        \"key\": \"/type/work\"\n    },\n    \"last_modified\": {\n        \"type\": \"/type/datetime\",\n        \"value\": \"2012-07-11T20:19:14.469842\"\n    },\n    \"covers\": [\n        2940133\n    ],\n    \"revision\": 4\n}"},{"cell_type":"markdown","id":"5b35a875-8245-462f-b02e-e0a9f97c9ac4","metadata":{},"source":"### Download Dataset\n\nI have hosted the slimmed down dataset on the [GitHub Repo](https://github.com/singlestore-labs/webinar-code-examples/tree/main/kai-product-rec) for this webinar. We'll download it from there.\n\nUsing the `requests` library, we will download the txt file, iterate through it in chunks, writing those chunks to a local file.\n\nBelow, you'll see us setting the output of the `download_file()` function to the variable `local_dataset`."},{"cell_type":"code","execution_count":null,"id":"8485cfce-9287-4513-a30b-bae0d60df011","metadata":{"tags":[],"trusted":true},"outputs":[],"source":"import requests\n\ndataset_url = 'https://raw.githubusercontent.com/singlestore-labs/webinar-code-examples/main/kai-product-rec/books_scifi.txt'\n\ndef download_file(dataset_url):\n    local_filename = dataset_url.split('/')[-1]\n    with requests.get(dataset_url, stream=True) as r:\n        r.raise_for_status()\n        with open(local_filename, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=8192): \n                # If you have chunk encoded response uncomment if\n                # and set chunk_size parameter to None.\n                #if chunk: \n                f.write(chunk)\n    return local_filename\n\nlocal_dataset = download_file(dataset_url)"},{"cell_type":"markdown","id":"08df354e-a408-4b83-955c-40eb75760d09","metadata":{},"source":"### Read text file into variable\n\nThe file created in the previous step will now be opened and read line-by-line into the variable `data`."},{"cell_type":"code","execution_count":null,"id":"a9583590-f3dc-4ec1-b325-4068177a9e02","metadata":{"tags":[],"trusted":true},"outputs":[],"source":"file = open(local_dataset)\ndata = file.readlines()\nfile.close()"},{"cell_type":"markdown","id":"5e764abb-eea8-486c-958a-7ae666aaad2d","metadata":{},"source":"### Track Token Usage\n\nQuerying OpenAI a bunch of times can get expensive, so you'll want to keep track of your token usage. In our case, I looked at the [Pricing Page](https://openai.com/pricing). specifically for the model that we will be using to create our vectors (`text-embedding-ada-002`).\n\nThe function below will take the dollar limit you set for the project and do the math to determine the max number of tokens you can expend, while the function (`budget_status()` will let us know throughout our project how we're doing on token usage."},{"cell_type":"code","execution_count":null,"id":"2594abfa-2268-4330-8b33-3fe1e3485c70","metadata":{"tags":[],"trusted":true},"outputs":[],"source":"cost_per_1k = 0.0001\ndollar_limit = 20.00\nbudget_tokens = (dollar_limit / cost_per_1k) * 1000\ntoken_usage = 0\n\ndef budget_status(token_usage):\n    if budget_tokens \u003e token_usage:\n        return 'ok'\n    else:\n        return 'spent'"},{"cell_type":"markdown","id":"48186a20-4e4a-44cd-8105-b82a9976cc1f","metadata":{"tags":[]},"source":"### Create SQL Table\n\nHere we are creating the table to store our books in. We will have an auto incrementing `_id` as the primary key, while storing the book title and the embedding generated from OpenAI."},{"cell_type":"code","execution_count":null,"id":"5908bfd7-e9bb-48f1-a1f0-a2b9d56896db","metadata":{"tags":[],"trusted":true},"outputs":[],"source":"%%sql\nCREATE TABLE IF NOT EXISTS products (\n  _id INT AUTO_INCREMENT PRIMARY KEY,\n  title VARCHAR(255) NOT NULL,\n  embedding BLOB NOT NULL\n);"},{"cell_type":"markdown","id":"2da984d0-6899-4150-b39a-1c246e386ece","metadata":{},"source":"### Create Embeddings and Load into S2\n\nThis is a fairly complex process, so we'll break everything out as best as we can here. Here are the high-level steps we need to cover:\n\n1. Install `openai` and import required libraries. Additionally set some default configuration settings and create the database connection.\n\n\u003e Note: `connection_url` is unique to SingleStore Notebooks, as it's a variable that contains the connection string for your databse.\n\n2. Create the functions needed to query OpenAI for the Embeddings and to Write to the Database.\n3. Loop through the books dataset (which is stored in `data`), calling for the creation of embedding, then store it to the database."},{"cell_type":"markdown","id":"fa2d683c-11b5-4559-8401-02bad5187cff","metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:52:37.630667Z","iopub.status.busy":"2023-06-27T18:52:37.630183Z","iopub.status.idle":"2023-06-27T18:52:37.633477Z","shell.execute_reply":"2023-06-27T18:52:37.632756Z","shell.execute_reply.started":"2023-06-27T18:52:37.630644Z"},"tags":[]},"source":"#### Step 1: Install and import libraries, adjust configuration"},{"cell_type":"code","execution_count":null,"id":"3e90767b-12c9-4511-bd66-2831691b9a83","metadata":{"tags":[],"trusted":true},"outputs":[],"source":"!pip install openai\n\nimport openai\nimport ast\nfrom sqlalchemy import *\n\nopenai.organization = OPENAI_ORG\nopenai.api_key = OPENAI_API_KEY\n\nconn = create_engine(connection_url)\nupdate_interval = 500 # How often to update you in the terminal of status\n\nmodel_id = 'text-embedding-ada-002'"},{"cell_type":"markdown","id":"29ec57d8-0c07-48c2-9c44-f0eadef55bdb","metadata":{},"source":"#### Step 2: Create the functions to query OpenAI and Write to Database"},{"cell_type":"code","execution_count":null,"id":"72e4b31a-970f-40e1-a9ca-ad085dcf4a0e","metadata":{},"outputs":[],"source":"ds_with_embeddings = []\ntotal_items = len(data)\n\ndef request_embedding(text, token_usage):\n    \n    budget = budget_status(token_usage)\n    \n    if budget == 'ok':\n        #print('Budget status: OK\\nTokens: {}/{}'.format(token_usage,budget_tokens))\n        try:\n            if OPENAI_API_KEY:\n                response = openai.Embedding.create(input=text,model=model_id)\n                embedding = response['data'][0]['embedding']\n                tokens = response['usage']['total_tokens']\n                status = 'success'\n                #print(embedding)\n                return embedding,tokens,status\n            else:\n                print('You need to set your OpenAI API Key to the variable OPENAI_API_KEY')\n        except Exception as e:\n            print(e)\n            embedding = ''\n            tokens = 0\n            status = 'failed'\n            return embedding,tokens,status\n    else:\n        print('Budget Spent: {}/{}'.format(token_usage,budget_tokens))\n        embedding = ''\n        tokens = 0\n        status = 'budget_spent'\n        return embedding,tokens,status\n\ndef write_to_db(data):\n    keys = [\"title\", \"embedding\" ];\n    query = \"INSERT INTO products (title, embedding) VALUES (%s, JSON_ARRAY_PACK_F32(%s))\"\n    \n    try:\n        with conn:\n            conn.execute(query, (data[keys[0]].replace(\"'\",\"\"), str(data[keys[1]])))\n            print(\"Wrote item\")\n    except Exception as e:\n        print(e)"},{"cell_type":"markdown","id":"b3882bc5-16c9-401d-8237-9cf06238643f","metadata":{},"source":"#### Step 4: Loop through dataset requesting embedding, append embedding to dataset, write dataset to database"},{"cell_type":"code","execution_count":null,"id":"527542d7-9732-43f1-bf65-b0864de90450","metadata":{},"outputs":[],"source":"loop_counter = 0\nprint('Requesting embeddings. I will update you every {} embeddings.'.format(str(update_interval))\nfor b in data:\n    try:\n        embedding,tokens,status = request_embedding(b, token_usage)\n        if status != 'failed' and status != 'budget_spent':\n            book = ast.literal_eval(b)\n            book['embedding'] = embedding\n            write_to_db(book)\n            token_usage += tokens\n            #print('Completed {}/{}'.format(len(ds_with_embeddings),total_items))\n            loop_counter += 1\n            if loop_counter == update_interval:\n                print('Completed {}/{}'.format(len(ds_with_embeddings),total_items))\n                print('Token usage: {}/{}'.format(token_usage,budget_tokens))\n                loop_counter = 0\n        elif status == 'budget_spent':\n            print('Getting embedding failed because the budget is spent.')\n        else:\n            print('Getting embedding for this book failed:\\n{}'.format(b))\n    except Exception as e:\n        print(e)\n        \nconn.close()"},{"cell_type":"code","execution_count":null,"id":"b23b6746-8a17-4441-abd4-9ae6d7d8b648","metadata":{},"outputs":[],"source":"query = 'The Martian'\n\n\n\nsql_query = 'SELECT title FROM products WHERE EUCLIDEAN_DISTANCE(vector, JSON_ARRAY_PACK('query')) ;\n\nSELECT EUCLIDEAN_DISTANCE(vector, JSON_ARRAY_PACK('[5.9,3,5.1,1.8]')) AS euclidean_distance, title\nFROM products\nORDER BY euclidean_distance\nLIMIT 5;\n\n"},{"cell_type":"code","execution_count":null,"id":"1906aa2c-8929-4e70-b64b-6843359323ec","metadata":{},"outputs":[],"source":""}],"metadata":{"jupyterlab":{"notebooks":{"version_major":6,"version_minor":4}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"singlestore_connection":{"connectionID":"00336d64-e4db-4635-aa2b-4ab9f330727a","defaultDatabase":"kai_product_rec"},"singlestore_row_limit":300},"nbformat":4,"nbformat_minor":5}